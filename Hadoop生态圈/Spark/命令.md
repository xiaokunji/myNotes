注:使用spark时,需要开启HDFS,(如果运行在yarn上还需开YARN)

启动Spark: (hadoop这个命令不起作用了)
`start-all.sh`

启动后主机有Master进程,
从机有Worker进程

停止Spark:
`stop-all.sh`

进入Spark-Shell:(进入scala环境)
`spark-shell`     

上传jar包:(这里是运行在Standalone上,--master后是指定资源管理器)  
`spark-submit --master spark://master:7077 --class com.yc.hello  hello.jar`

更多命令:`http://dataunion.org/10345.html`
