[toc]

# 主从复制
**复制过程**:
1. 从服务器连接主服务器，发送SYNC（同步）命令；
2. 主服务器接收到SYNC命名后，开始执行BGSAVE命令生成RDB文件并使用缓冲区记录此后执行的所有写命令；
3. 主服务器BGSAVE执行完后，向所有从服务器发送快照文件，并在发送期间继续记录被执行的写命令；
4. 从服务器收到快照文件后丢弃所有旧数据，载入收到的快照；
5. 主服务器快照发送完毕后开始向从服务器发送缓冲区中的写命令；
6. 从服务器完成对快照的载入，开始接收命令请求，并执行来自主服务器缓冲区的写命令；（从服务器初始化完成）
7. 主服务器每执行一个写命令就会向从服务器发送相同的写命令，从服务器接收并执行收到的写命令（从服务器初始化完成后的操作）

**优点**:

1. <u>为了分载Master的读操作压力，Slave服务器可以为客户端提供只读操作的服务，写服务仍然必须由Master来完成</u>
2. Slave同样可以接受其它Slaves的连接和同步请求，这样可以有效的分载Master的同步压力。
3. Master Server是以非阻塞的方式为Slaves提供服务。所以在Master-Slave同步期间，客户端仍然可以提交查询或修改请求。
4. Slave Server同样是以非阻塞的方式完成数据同步。在同步期间，如果有客户端提交查询请求，Redis则返回同步之前的数据

**缺点**
1. Redis不具备自动容错和恢复功能，++主机从机的宕机都会导致前端部分读写请求失败，需要等待机器重启或者手动切换前端的IP才能恢复。++
2. 主机宕机，宕机前有部分数据未能及时同步到从机，切换IP后还会引入数据不一致的问题，降低了系统的可用性。
3. Redis较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。

链接：https://www.jianshu.com/p/7967f95655b2
> 主从复制是基础,后面的哨兵和Cluster都是基于这种方式,但是这种方式太垃圾了,master死了就不能写操作了,所以一般会用后面两种
使用主从复制很简单,在配置文件中指定master/slave,并设置密码(必须使用)

# 哨兵模式
哨兵的作用就是监控Redis系统的运行状况。它的功能包括以下两个。
1. 监控主服务器和从服务器是否正常运行。
2. 主服务器出现故障时自动将从服务器转换为主服务器。
> 弥补了主从模式下的master死亡的情况

哨兵是集群,有主节点和从节点,也会产生选举(基于Raft算法(但不一样),和kafka一样),会选举哨兵主节点,redis的master,( 每个redis的bin下都有一个哨兵启动文件),在sentinel模式下,客户端就不用直接连接Redis，而是连接sentinel的ip和port

> 哨兵是一个独立的进程，作为进程，它会独立运行
>
> **每个哨兵通过主节点去发现其他从节点和其他哨兵的**

![image](https://upload-images.jianshu.io/upload_images/11320039-3f40b17c0412116c.png?imageMogr2/auto-orient/strip|imageView2/2/w/747/format/webp)

**工作方式**

* 每个sentinel以每秒钟一次的频率向它所知的master，slave以及其他sentinel实例发送一个 PING 命令 
* 如果一个实例距离最后一次有效回复 PING 命令的时间超过 down-after-milliseconds 选项所指定的值， 则这个实例会被sentinel标记为**主观下线**。 
* 如果一个master被标记为主观下线，则正在监视这个master的所有sentinel要以每秒一次的频率(发送info命令)确认master的确进入了主观下线状态
* 当有足够数量(`quorum`)的sentinel在30s（`down-after-milliseconds`）内确认master的确进入了主观下线状态， 则master会被标记为**客观下线** 
* 在一般情况下， 每个sentinel会以每 10 秒一次的频率向它已知的所有master，slave发送 INFO 命令 
* 当master被sentinel标记为客观下线时，sentinel向下线的master的所有slave发送 INFO 命令的频率会从 10 秒一次改为 1 秒一次 
* 若没有足够数量的sentinel同意master已经下线，master的客观下线状态就会被移除； 若master重新向sentinel的 PING 命令返回有效回复，master的主观下线状态就会被移除

>redis自带哨兵,不需要借助第三方,只需要简单配置即可 ,生产环境建议让redis Sentinel部署到不同的物理机上。  
>
>原文链接：https://blog.csdn.net/miss1181248983/article/details/90056960

>哨兵可以解决大量读的问题,如果有大量写,哨兵也扛不住,这需要Twemproxy,做数据的分片处理,再配合 Lvs 和 Keepalived 解决Twemproxy的单点故障   
>
>https://www.jianshu.com/p/84dbb25cc8dc



**哨兵与各节点**

> 哨兵和master通过配置文件直连

1. Sentinel 自动发现 Slave
   	- 每 10 秒 Sentinel 向 master 节点发送 INFO 命令后获取到所有 slave 的信息
      	- Sentinel 与 slave 建立命令连接和订阅连接

2. sentinel 自动发现机制
   - Sentinel 利用 pub/sub（发布/订阅）机制，订阅了每个 master 和 slave 数据节点的 `__sentinel__:hello` 频道，去自动发现其它也监控了统一 master 的 sentinel 节点
   - Sentinel 向每 1s 向 `__sentinel__:hello` 中发送一条消息，包含了其当前维护的最新的 master 配置。如果某个sentinel发现自己的配置版本低于接收到的配置版本，则会用新的配置更新自己的 master 配置
   - 与发现的 Sentinel 之间相互建立命令连接，之后会通过这个命令连接来交换对于 master 数据节点的看法

> 建立两个异步网络连接：
>
> 1. 命令连接：用于向 Redis master 数据节点发送命令，例如通过 INFO 命令了解：
>    - master 本身运行信息，用于更新本地的 master 字典（Redis Hash 的实现中用到字典使用的的也是这个数据结构）
>    - slaves 信息（角色、IP、Port、连接状态、优先级、复制偏移量），用于更新本地的 slave 字典
>
> 2. 订阅连接：订阅 `__sentinel__:hello` 频道，用于发现其他 Sentinel，频道中信息包括：
>    - Sentinel 自身信息（IP、Port、RunID、Epoch）
>    - 监视的 Master 节点的信息（Name、IP、Port、Epoch）
>
> [Redis 哨兵模式(Sentinel) 原理 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/201534164)



**选举过程**

1. 先选举出sentinel中的leader

   > 用的过半选举那套,最终决定leader时是 leader最低票数( `quorum`和`Sentinel`节点数/2+1的最大值), 所以推荐sentinel要部署奇数台

2. 然后又sentinel的leader选举slave为master

   - 过滤故障的节点
   - 选择优先级slave-priority最大的从节点作为主节点，如不存在则继续
   - 选择复制偏移量（数据写入量的字节，记录写了多少数据。主服务器会把偏移量同步给从服务器，当主从的偏移量一致，则数据是完全同步）最大的从节点作为主节点，如不存在则继续
   - 选择runid（redis每次启动的时候生成随机的runid作为redis的标识）最小的从节点作为主节点

[redis哨兵模式选举机制_LiaoHongHB的博客-CSDN博客_redis哨兵模式选举机制](https://blog.csdn.net/LiaoHongHB/article/details/109092165)

[Raft协议实战之Redis Sentinel的选举Leader源码解析_xuhao_xuhao的专栏-CSDN博客](https://blog.csdn.net/xuhao_xuhao/article/details/78885752)

# Cluster模式
redis的哨兵模式基本已经可以实现高可用，读写分离 ，但是在这种模式下每台redis服务器都存储相同的数据，很浪费内存，所以在redis3.0上加入了cluster模式，实现的redis的分布式存储，也就是说每台redis节点上存储不同的内容。(取数据的时候直接找到对应地址去取)

默认情况下，redis集群的**读和写都是到master上去执行的**，不支持slave节点读和写，跟Redis主从复制下读写分离不一样，因为redis集群的核心的理念，主要是使用slave做数据的热备，以及master故障时的主备切换，实现高可用的。

Redis-Cluster采用无中心结构,它的特点如下

1.  所有的redis节点彼此互联(PING-PONG机制),内部使用二进制协议优化传输速度和带宽。
2. 节点的fail是通过集群中超过半数的节点检测失效时才生效。
3.  客户端与redis节点直连,不需要中间代理层.客户端不需要连接集群所有节点,连接集群中任何一个可用节点即可。

**工作方式**   

在redis的每一个节点上，都有这么两个东西，一个是插槽（slot），它的的取值范围是：0-16383。还有一个就是cluster，可以理解为是一个集群管理的插件。<u>当我们的存取的key到达的时候，redis会根据crc16的算法得出一个结果</u>，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，通过这个值，<u>去找到对应的插槽所对应的节点</u>，然后直接自动跳转到这个对应的节点上进行存取操作。每个节点负责其中一部分槽位。槽位的信息存储于每个节点中。只有master节点会被分配槽位，slave节点不会分配槽位。

当Redis Cluster 的客户端来连接集群时，它也会得到一份集群的槽位配置信息，并将其缓存在客户端本地。这样当客户端要查找某个 key 时，可以直接定位到目标节点。同时因为槽位的信息可能会存在客户端与服务器不一致的情况，还需要纠正机制来实现槽位信息的校验调整。

> 客户端会存一份槽位的分布信息 , 如果槽位信息不对(例如槽位发生了迁移),客户端会更新为最新的槽位信息.
>
> 也就意味着, **客户端一般能直接命中key,不会发生二次访问**
>
> 这个客户端是 smart客户端，就是指客户端本地维护一份hashslot => node的映射表缓存，大部分情况下，直接走本地缓存就可以找到hashslot => node，不需要通过节点进行moved重定向，(jedisCluster就集成了它)

**槽位定位算法**

![](https://pic3.zhimg.com/v2-348db90c6d12f589f5848ac273c17486_r.jpg)



[【原创】为什么Redis集群有16384个槽 - 孤独烟 - 博客园 (cnblogs.com)](https://www.cnblogs.com/rjzheng/p/11430592.html)

> 16384 / 8= 2048 (B)
>
> 因为集群节点通信需要带上当前节点槽位信息, 槽位信息存于一个char数组就是`集群槽位数/8`, 数组中的每一位表示一个槽, 其值为1表示拥有该槽
>
> 所以槽位不宜过大, 不然节点间通信心跳包过大
>
> ![img](https://img2018.cnblogs.com/blog/725429/201908/725429-20190829164739879-973731722.jpg)

总结: 

1. 如果槽位为65536，发送心跳信息的消息头达8k，发送的心跳包过于庞大。 `65536÷8÷1024=8kb`
2. redis的集群主节点数量基本不可能超过1000个。槽位越小，节点少的情况下，压缩比高



**跳转重定位**

当客户端向一个节点发出了指令，首先当前节点会计算指令的 key 得到槽位信息，判断计算的槽位是否归当前节点所管理；若槽位不归当前节点管理，这时它会向客户端发送一个特殊的跳转指令携带目标操作的节点地址，告诉客户端去连这个节点去获取数据。客户端收到指令后除了跳转到正确的节点上去操作，还会同步更新纠正本地的槽位映射表缓存，后续所有 key 将使用新的槽位映射表。

![](https://pic3.zhimg.com/v2-7577727ef1b7df91520a51fd20b589b6_r.jpg)

> 直接用控制台发送命令 就会返回真实的key所在ip



为了保证高可用，redis-cluster集群引入了主从模式，一个主节点对应一个或者多个从节点，当主节点宕机的时候，就会启用从节点。当其它主节点ping一个主节点A时，如果半数以上的主节点与A通信超时，那么认为主节点A宕机了。如果主节点A和它的从节点A1都宕机了，那么该集群就无法再提供服务了 

> 当一个主节点和其从节点都宕机时, cluster-require-full-coverage配置解决该集群是否可用, 默认为 yes, 表示需要主节点必须全存活



**Redis集群所有节点之间的通信机制**

在Redis集群中，不同的节点之间采用gossip协议进行通信，节点之间通讯的目的是为了维护节点之间的元数据信息。这些元数据就是每个节点包含哪些数据，是否出现故障，通过gossip协议，达到最终数据的一致性。

> gossip协议，是基于流行病传播方式的节点或者进程之间信息交换的协议。原理就是在不同的节点间不断地通信交换信息，一段时间后，所有的节点就都有了整个集群的完整信息，并且所有节点的状态都会达成一致。每个节点会携带集群节点总数的1/10(至少3个)的信息，但只要这些节可以通过网络连通，最终他们的状态就会是一致的。Gossip协议最大的好处在于，即使集群节点的数量增加，每个节点的负载也不会增加很多，几乎是恒定的。
>
> 也意味着元数据的更新有延时，可能导致集群中的一些操作会有一些滞后, 同时结点数太多，意味着达到最终一致性的时间也相对变长，因此官方推荐最大节点数为1000左右。
>
> redis cluster架构下的每个redis都要开放两个端口号，比如一个是6379，另一个就是加10000的端口号16379。
>
> - 6379端口号就是redis服务器入口。
> - 16379端口号是用来进行节点间通信的，也就是 cluster bus 的东西，cluster bus 的通信，用来进行故障检测、配置更新、故障转移授权。cluster bus 用的是一种叫gossip 协议的二进制协议
>
> 更多知识详见: [Redis集群原理详解_张维鹏的博客-CSDN博客_redis集群原理](https://blog.csdn.net/a745233700/article/details/112691126)



> 维护集群的元数据有集中式和 gossip两种方式, 像zookeeper就是集中式
>
> [Redis集群搭建及原理，肝了！ - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/377966104)



**gossip协议的常见类型：**

gossip协议常见的消息类型包含： ping、pong、meet、fail等等。

- meet：主要用于通知新节点加入到集群中

- ping：用于交换节点的元数据

- pong：ping和meet消息的响应，同样包含了自身节点的状态和集群元数据信息。

- fail：某个节点判断另一个节点 fail 之后，向集群所有节点广播该节点挂掉的消息，其他节点收到消息后标记已下线。

  > ​		由于Redis集群的去中心化以及gossip通信机制，Redis集群中的节点只能保证最终一致性。例如当加入新节点时(meet)，只有邀请节点和被邀请节点知道这件事，其余节点要等待 ping 消息一层一层扩散。除了 Fail 是立即全网通知的，其他诸如新节点、节点重上线、从节点选举成为主节点、槽变化等，都需要等待被通知到，也就是Gossip协议是最终一致性的协议。



**集群的扩容与收缩：**

1. **扩容**

   （1）启动新节点
   （2）使用cluster meet命令将新节点加入到集群
   （3）迁移槽和数据：添加新节点后，需要将一些槽和数据从旧节点迁移到新节点

新节点加入到集群的时候，作为孤儿节点是没有和其他节点进行通讯的。因此需要在集群中任意节点执行 cluster meet 命令让新节点加入进来。假设新节点是 192.168.1.1 5002，老节点是 192.168.1.1 5003，那么运行以下命令将新节点加入到集群中。

`192.168.1.1 5003> cluster meet 192.168.1.1 5002`

这个是由老节点发起的，有点老成员欢迎新成员加入的意思。新节点刚刚建立没有建立槽对应的数据，也就是说没有缓存任何数据。如果这个节点是主节点，需要对其进行槽数据的扩容；如果这个节点是从节点，就需要同步主节点上的数据。总之就是要同步数据。

![](https://img-blog.csdnimg.cn/20210215043619589.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2E3NDUyMzM3MDA=,size_16,color_FFFFFF,t_70)



> **实际操作中, 添加主节点命令如下:**
>
> `./src/redis-cli --cluster add-node 172.26.237.83:7002 172.26.237.83:7000 -a 0123456789`
>
> <u>在命令中(给这个新节点)分配槽点数,</u>
>
> > 第一个ip:port 为需要添加的节点ip和端口，第二个ip:port为当前集群中的节点和端口；
>
> **添加从节点**
>
> `./src/redis-cli --cluster add-node --cluster-slave --cluster-master-id db10a9d5c1662d9e3cee21c5776f2e9709f76619 127.0.0.1:7008 127.0.0.1:7007`
>
> > 节点ID是主节点的ID
> >
> >  127.0.0.1:7008 是新加的从节点
> >
> > 127.0.0.1:7007 作为从节点的主节点
>
> [redis集群扩容（添加新节点） - 北向。 - 博客园 (cnblogs.com)](https://www.cnblogs.com/yfacesclub/p/11860927.html)
>
> [Redis集群增加节点和删除节点 - 全me村的希望 - 博客园 (cnblogs.com)](https://www.cnblogs.com/hopeofthevillage/p/11535683.html)



2. **收缩**

   （1）迁移槽
   （2）忘记节点。通过命令 cluster forget {downNodeId} 通知其他的节点

为了安全删除节点，<u>Redis集群只能下线没有负责槽的节点</u>。因此如果要下线有负责槽的master节点，则需要先将它负责的槽迁移到其他节点。迁移的过程也与上线操作类似，不同的是下线的时候需要通知全网的其他节点忘记自己，此时通过命令 cluster forget {downNodeId} 通知其他的节点。



**集群的故障检测与故障转恢复机制：**

1. **故障检测**

   和哨兵一样, 都存在主观下线和客观下线

   节点A访问节点B超时了,则会认为主观下线,当超过半数的主节点都认为节点B都是客观下线了

   接着向集群广播一条主节点B的Fail 消息，所有收到消息的节点都会标记节点B为客观下线。

2. **故障恢复**

   ​	当故障节点下线后，如果是持有槽的主节点则需要在其从节点中找出一个替换它，从而保证高可用。此时下线主节点的所有从节点都担负着恢复义务，这些从节点会定时监测主节点是否进入客观下线状态，如果是，则触发故障恢复流程。故障恢复也就是选举一个节点充当新的master，选举的过程是基于Raft协议选举方式来实现的。

   > 从节点并不是在主节点一进入 FAIL 状态就马上尝试发起选举，而是有一定延迟，一定的延迟确保我们等待FAIL状态在集群中传播
   >
   > 延迟计算公式：DELAY = 500ms + random(0 ~ 500ms) + SLAVE_RANK * 1000ms
   >
   > 也就是说, 持有最新数据的slave将会首先发起选举（理论上）, 则更大可能成为主节点

   2.1、从节点过滤：

   检查每个slave节点与master节点断开连接的时间，如果超过了cluster-node-timeout * cluster-slave-validity-factor，那么就没有资格切换成master

   > 就是说, slave和master节点太久没有连接了,就没有资格成为slave

   2.2、投票选举：

   （1）节点排序：

   对通过过滤条件的所有从节点进行排序，按照priority、offset、run id排序，排序越靠前的节点，越优先进行选举。

   > - replica-priority 的值越低，优先级越高 (默认值是100)
   > - offset越大，表示从master节点复制的数据越多，选举时间越靠前，优先进行选举
   > - 如果offset相同，run id越小，优先级越高

   （2）更新配置纪元：

   ​	每个主节点会去更新配置纪元（clusterNode.configEpoch），这个值是不断增加的整数。这个值记录了每个节点的版本和整个集群的版本

   （3）发起选举：

   ​		更新完配置纪元以后，从节点会向集群发起广播选举的消息（CLUSTERMSG_TYPE_FAILOVER_AUTH_REQUEST），要求所有收到这条消息，并且具有投票权的主节点进行投票。每个从节点在一个纪元中只能发起一次选举。

   （4）选举投票：

   ​		如果一个主节点具有投票权，并且这个主节点尚未投票给其他从节点，那么主节点将向要求投票的从节点返回一条`CLUSTERMSG_TYPE_FAILOVER_AUTH_ACK`消息，表示这个主节点支持从节点成为新的主节点。

   ​		每个参与选举的从节点都会接收`CLUSTERMSG_TYPE_FAILOVER_AUTH_ACK`消息，并根据自己收到了多少条这种消息来统计自己获得了多少主节点的支持。

   如果超过(N/2 + 1)数量的master节点都投票给了某个从节点，那么选举通过，这个从节点可以切换成master，如果在 cluster-node-timeout*2 的时间内从节点没有获得足够数量的票数，本次选举作废，更新配置纪元，并进行第二轮选举，直到选出新的主节点为止

2.3、替换主节点：

当满足投票条件的从节点被选出来以后，会触发替换主节点的操作。删除原主节点负责的槽数据，把这些槽数据添加到自己节点上，并且广播让其他的节点都知道这件事情，新的主节点诞生了。

（1）被选中的从节点执行SLAVEOF NO ONE命令，使其成为新的主节点

（2）新的主节点会撤销所有对已下线主节点的槽指派，并将这些槽全部指派给自己

（3）新的主节点对集群进行广播PONG消息，告知其他节点已经成为新的主节点

（4）新的主节点开始接收和处理槽相关的请求



**mget问题**

现象: 使用mget时性能出现明显下降

原因: 要查的key分布在多个节点上,所以需要给多个节点发送请求且为串行, 我们使用了一个mget,实际上访问了很多次

![img](https://segmentfault.com/img/remote/1460000040550894)

[浅谈Redis集群下mget的性能问题 - SegmentFault 思否](https://segmentfault.com/a/1190000040550883)



**Redis集群的运维：**

1. 数据迁移问题：

   Redis集群可以进行节点的动态扩容缩容，这一过程目前还处于半自动状态，需要人工介入。

2. 带宽消耗问题：

   Redis集群是无中心节点的集群架构，依靠Gossip协议协同自动化修复集群的状态，但goosip有消息延时和消息冗余的问题，在集群节点数量过多的时候，goosip协议通信会消耗大量的带宽

3. Pub/Sub广播问题：

   集群模式下内部对所有publish命令都会向所有节点进行广播，加重带宽负担，所以集群应该避免频繁使用Pub/sub功能

4. 集群倾斜：

   集群倾斜是指不同节点之间数据量和请求量出现明显差异，这种情况将加大负载均衡和开发运维的难度。因此需要理解集群倾斜的原因

   （1）数据倾斜：

   ​		节点和槽分配不均
   ​		不同槽对应键数量差异过大
   ​		集合对象包含大量元素
   ​		内存相关配置不一致
   （2）请求倾斜：

   ​		合理设计键，热点大集合对象做拆分或者使用hmget代替hgetall避免整体读取
   
   > [【原创】谈谈redis的热key问题如何解决 - 孤独烟 - 博客园 (cnblogs.com)](https://www.cnblogs.com/rjzheng/p/10874537.html)
   >
   > 1. 使用二级缓存(@enableCaching)
   > 2. 将key多备份几个,加个后缀名区分,这样就会随机的分布到其他机器
   >
   > 





>启用集群模式也是通过配置即可,这里只是简述,分布数据以及后续内容远不止这些
>
>链接：https://www.jianshu.com/p/7967f95655b2    
>
>https://blog.csdn.net/miss1181248983/article/details/90056960    
>
>https://www.cnblogs.com/williamjie/p/11132211.html
>
>[Redis集群原理详解_张维鹏的博客-CSDN博客_redis集群原理](https://blog.csdn.net/a745233700/article/details/112691126)
>
>[Redis集群搭建及原理，肝了！ - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/377966104)
>
>扩展阅读:
>
>[Redis哈希槽，对于哈希槽的理解，以及高并发情况下哈希槽不够的情况讲解，热点缓存的解决思路_ck784101777的博客-CSDN博客_redis哈希槽](https://blog.csdn.net/ck784101777/article/details/101367821)

